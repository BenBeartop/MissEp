#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å‰§é›†ç¼ºé›†æ£€æŸ¥å·¥å…·

è¯¥ç¨‹åºç”¨äºæ‰«æåª’ä½“åº“ä¸­çš„å‰§é›†,ä¸TMDBæ•°æ®è¿›è¡Œæ¯”è¾ƒ,æ‰¾å‡ºç¼ºå¤±çš„é›†æ•°,
å¹¶å¯é€‰æ‹©æ€§åœ°é€šè¿‡MoviePilotè¿›è¡Œè®¢é˜…æˆ–ä¸‹è½½ã€‚
"""

import os
import time
import asyncio
import aiofiles
import argparse
import logging
from typing import Dict, List, Set, Optional
from pathlib import Path
from collections import defaultdict

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from utils.config import (
    REPORT_FILE, SKIPPED_LOG, MAX_SHOWS,
    AUTO_SUBSCRIBE, AUTO_DOWNLOAD, SUBSCRIBE_THRESHOLD,
    STORAGE_TYPE, StorageType
)
from utils.helpers import (
    log_skipped, parse_filename, CacheData,
    load_cache, save_cache, reset_cache, merge_old_cache,
    async_error_handler, MediaProcessError
)
from storage import get_storage_backend
from tmdb.api import search_tv_show, get_tmdb_structure
from media_manager.moviepilot import (
    login as mp_login,
    handle_missing_episodes,
    MoviePilotResult
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@async_error_handler("Process")
async def process_show(
    dir_name: str, 
    cache: CacheData, 
    report_file: str, 
    is_specific_show: bool = False
) -> tuple:
    """å¤„ç†å•ä¸ªå‰§é›†å¹¶è¾“å‡ºç¼ºå¤±"""
    # æ£€æŸ¥è¯¥å‰§é›†æ˜¯å¦å·²ç»å®Œæ•´,ä½†å¦‚æœæ˜¯æŒ‡å®šè¦æ£€æŸ¥çš„å‰§é›†åˆ™ä¸è·³è¿‡
    if cache.is_complete_dir(dir_name) and not is_specific_show:
        tmdb_id = cache.complete_dirs[dir_name]["tmdb_id"]
        print(f"  âœ… å‰§é›†å·²å®Œæ•´ [TMDB ID: {tmdb_id}]ï¼Œè·³è¿‡æ£€æŸ¥")
        return 0, 0, False, False
    
    print(f"\nå¤„ç†å‰§é›†: {dir_name}")
    
    local_seasons = defaultdict(set)
    skipped_files = []
    
    # è·å–å­˜å‚¨åç«¯
    storage = get_storage_backend()
    
    # è·å–æ–‡ä»¶å¹¶è§£æ
    try:
        files = await storage.get_directory_structure(dir_name)
        if not files:
            print(f"  æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶")
            return 0, 0, False, False
    except Exception as e:
        print(f"  è·å–ç›®å½•ç»“æ„å¤±è´¥: {e}")
        return 0, 0, False, False
    
    # è§£ææ–‡ä»¶åï¼Œè·å–å­£é›†ä¿¡æ¯
    valid_count = 0
    
    for file_info in files:
        filepath = file_info["path"]
        known_season = file_info["season"]
        
        parsed = parse_filename(filepath, known_season)
        if parsed:
            season, episode = parsed
            local_seasons[season].add(episode)
            valid_count += 1
        else:
            skipped_files.append(filepath)
    
    if skipped_files:
        for file in skipped_files:
            await log_skipped(f"{dir_name}/{file}")
    
    if not any(local_seasons.values()):
        print(f"  æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆå‰§é›†æ–‡ä»¶")
        return 0, len(skipped_files), False, False
    
    # æŸ¥è¯¢TMDBå¹¶éªŒè¯å¹´ä»½
    try:
        tmdb_id, tmdb_name = await search_tv_show(dir_name, cache)
        if not tmdb_id:
            print(f"  TMDB æŸ¥è¯¢å¤±è´¥ï¼Œæœªæ‰¾åˆ°è¯¥å‰§")
            return valid_count, len(skipped_files), False, False
        
        # è·å–TMDBå‰§é›†ç»“æ„
        tmdb_structure = await get_tmdb_structure(tmdb_id)
        if not tmdb_structure:
            print(f"  TMDB æœªè¿”å›å­£é›†ä¿¡æ¯")
            return valid_count, len(skipped_files), False, False
    except Exception as e:
        print(f"  æŸ¥è¯¢TMDBä¿¡æ¯å¤±è´¥: {e}")
        return valid_count, len(skipped_files), False, False
    
    # æ£€æŸ¥ç¼ºå¤±å‰§é›†
    missing = defaultdict(list)
    for season in tmdb_structure:
        all_eps = set(tmdb_structure[season])
        local_eps = local_seasons.get(season, set())
        diff = all_eps - local_eps
        if diff:
            missing[season] = sorted(list(diff))
    
    # åªå¤„ç†æœ‰ç¼ºå¤±çš„å‰§é›†
    if not missing:
        print(f"  âœ… æ‰€æœ‰å­£/é›†é½å…¨")
        # è®°å½•å®Œæ•´å‰§é›†ä¿¡æ¯åˆ°ç¼“å­˜
        cache.mark_complete_dir(dir_name, tmdb_id)
        await save_cache(cache)
        return valid_count, len(skipped_files), False, True
    else:
        print(f"  âŒ å‘ç°ç¼ºå¤±å‰§é›†:")
        result_lines = []
        
        result_lines.append(f"\nã€{dir_name}ã€‘ - {tmdb_name} [TMDB ID: {tmdb_id}]")
        result_lines.append(f"  å·²æœ‰å‰§é›†: {dict(local_seasons)}")
        result_lines.append(f"  TMDBå‰§é›†ç»“æ„: {dict((s, len(e)) for s, e in tmdb_structure.items())}")
        
        # å¤„ç†ç¼ºå¤±å‰§é›†
        mp_results = []
        
        for season in sorted(missing):
            miss_str = f"  âŒ Season {season}: ç¼ºå°‘é›†æ•° {missing[season]}"
            print(miss_str)
            result_lines.append(miss_str)
            
            # ä½¿ç”¨MoviePilotå¤„ç†ç¼ºå¤±å‰§é›†
            if AUTO_SUBSCRIBE or AUTO_DOWNLOAD:
                try:
                    mp_result = await handle_missing_episodes(
                        show_name=dir_name,
                        tmdb_id=tmdb_id,
                        season=season,
                        episodes=missing[season],
                        auto_subscribe=AUTO_SUBSCRIBE,
                        auto_download=AUTO_DOWNLOAD,
                        subscribe_threshold=SUBSCRIBE_THRESHOLD
                    )
                    result_str = f"  ğŸ¬ Season {season}: {mp_result.message}"
                    print(result_str)
                    mp_results.append(result_str)
                except Exception as e:
                    error_str = f"  âŒ Season {season}: å¤„ç†å‡ºé”™ - {e}"
                    print(error_str)
                    mp_results.append(error_str)
        
        # æ·»åŠ MoviePilotå¤„ç†ç»“æœ
        if mp_results:
            result_lines.append("  --MoviePilotå¤„ç†ç»“æœ--")
            result_lines.extend(mp_results)
        
        async with aiofiles.open(report_file, "a", encoding="utf-8") as f:
            await f.write("\n".join(result_lines) + "\n")
            
        return valid_count, len(skipped_files), True, False

@async_error_handler("Main")
async def main_async(specific_show=None):
    """ä¸»å¼‚æ­¥å‡½æ•°"""
    start_time = time.time()
    print("å¼€å§‹æ£€æŸ¥ç¼ºå¤±å‰§é›†...")
    
    # é…ç½®MoviePilot
    if AUTO_SUBSCRIBE or AUTO_DOWNLOAD:
        print("æ­£åœ¨åˆå§‹åŒ–MoviePilot...")
        try:
            mp_success = await mp_login()
            if mp_success:
                print("MoviePilotå·²å‡†å¤‡å°±ç»ª")
                print(f"è‡ªåŠ¨è®¢é˜…åŠŸèƒ½: {'å¼€å¯' if AUTO_SUBSCRIBE else 'å…³é—­'}")
                if AUTO_SUBSCRIBE:
                    if SUBSCRIBE_THRESHOLD == 0:
                        print(f"è®¢é˜…ç­–ç•¥: æ— è®ºç¼ºå¤±å‡ é›†éƒ½è®¢é˜…")
                    else:
                        print(f"è®¢é˜…ç­–ç•¥: ç¼ºå¤±è¶…è¿‡{SUBSCRIBE_THRESHOLD}é›†æ—¶è®¢é˜…æ•´å­£")
                print(f"è‡ªåŠ¨ä¸‹è½½åŠŸèƒ½: {'å¼€å¯' if AUTO_DOWNLOAD else 'å…³é—­'}")
            else:
                print("MoviePilotç™»å½•å¤±è´¥,è‡ªåŠ¨è®¢é˜…å’Œä¸‹è½½åŠŸèƒ½å°†ä¸å¯ç”¨")
        except Exception as e:
            print(f"MoviePilotåˆå§‹åŒ–å¤±è´¥: {e}")
    
    # æ¸…é™¤ä¸Šæ¬¡çš„æ–‡ä»¶
    if not specific_show:
        if os.path.exists(SKIPPED_LOG):
            os.unlink(SKIPPED_LOG)
        
        if os.path.exists(REPORT_FILE):
            os.unlink(REPORT_FILE)
        
        # åˆå§‹åŒ–æŠ¥å‘Šæ–‡ä»¶
        async with aiofiles.open(REPORT_FILE, "w", encoding="utf-8") as f:
            await f.write(f"åª’ä½“åº“ç¼ºå¤±å‰§é›†æŠ¥å‘Š (å­˜å‚¨ç±»å‹: {STORAGE_TYPE.value})\n===============================\n")
    
    # åŠ è½½ç»Ÿä¸€ç¼“å­˜
    cache = await load_cache()
    
    # è·å–å®Œæ•´ç›®å½•æ•°é‡
    complete_count = len(cache.complete_dirs)
    print(f"å·²åŠ è½½ç¼“å­˜: {len(cache.tmdb_map)} ä¸ªå‰§é›†æ˜ å°„, {complete_count} ä¸ªå®Œæ•´å‰§é›†è®°å½•")
    
    # æ˜¾ç¤ºç¼ºå¤±å‰§é›†çš„åˆ—è¡¨æ ‡é¢˜
    if not specific_show:
        async with aiofiles.open(REPORT_FILE, "a", encoding="utf-8") as f:
            await f.write("\nç¼ºå¤±å‰§é›†åˆ—è¡¨\n-----------------\n")
    
    new_complete_count = 0
    
    # åˆå§‹åŒ–å­˜å‚¨åç«¯
    storage = get_storage_backend()
    print(f"ä½¿ç”¨å­˜å‚¨åç«¯: {STORAGE_TYPE.value}")
    
    if specific_show:
        # ä»…å¤„ç†æŒ‡å®šçš„å‰§é›†
        dirs = [specific_show]
        print(f"ä»…å¤„ç†æŒ‡å®šçš„å‰§é›†: {specific_show}")
    else:
        # è·å–æ‰€æœ‰å‰§é›†ç›®å½•
        try:
            dirs = await storage.list_directories()
            
            if not dirs:
                print("æœªæ‰¾åˆ°ä»»ä½•å‰§é›†ç›®å½•")
                return
            
            total_dirs = len(dirs)
            print(f"æ‰¾åˆ° {total_dirs} ä¸ªå‰§é›†ç›®å½•")
            
            # é™åˆ¶å¤„ç†çš„å‰§é›†æ•°é‡
            if MAX_SHOWS:
                dirs = dirs[:MAX_SHOWS]
                print(f"å°†ä»…å¤„ç†å‰ {len(dirs)} ä¸ªå‰§é›†")
        except Exception as e:
            print(f"è·å–ç›®å½•åˆ—è¡¨å¤±è´¥: {e}")
            return
    
    total_processed = 0
    total_skipped = 0
    total_missing = 0
    
    # æŒ‰ç›®å½•å¤„ç†ï¼Œå®æ—¶è¾“å‡º
    for index, dir_name in enumerate(dirs, 1):
        try:
            if len(dirs) > 1:
                print(f"\n[{index}/{len(dirs)}] å¤„ç†å‰§é›†: {dir_name}")
            else:
                print(f"\nå¤„ç†å‰§é›†: {dir_name}")
            
            processed, skipped, has_missing, is_complete = await process_show(
                dir_name, 
                cache, 
                REPORT_FILE, 
                is_specific_show=bool(specific_show)
            )
            total_processed += processed
            total_skipped += skipped
            if has_missing:
                total_missing += 1
            if is_complete:
                new_complete_count += 1
            
            # æ¯å¤„ç†10ä¸ªå‰§é›†ï¼Œä¿å­˜ä¸€æ¬¡ç¼“å­˜
            if index % 10 == 0 and len(dirs) > 10:
                await save_cache(cache)
                print(f"å·²å¤„ç† {index}/{len(dirs)} ä¸ªå‰§é›†...")
        except Exception as e:
            print(f"å¤„ç†å‰§é›† {dir_name} æ—¶å‡ºé”™: {e}")
            continue
    
    elapsed = time.time() - start_time
    
    # å®Œæˆå¤„ç†
    print("\næ£€æŸ¥å®Œæˆ!")
    if not specific_show:
        print(f"æ€»å…±å¤„ç†äº† {len(dirs)} ä¸ªå‰§é›†ç›®å½•")
        print(f"æ‰¾åˆ° {total_missing} ä¸ªæœ‰ç¼ºå¤±çš„å‰§é›†")
        print(f"æ–°å¢ {new_complete_count} ä¸ªå®Œæ•´å‰§é›†è®°å½•")
        print(f"æ€»å…±å¤„ç†äº† {total_processed} ä¸ªæœ‰æ•ˆæ–‡ä»¶")
        print(f"æ€»å…±è·³è¿‡äº† {total_skipped} ä¸ªæ— æ³•è¯†åˆ«çš„æ–‡ä»¶")
        print(f"è€—æ—¶: {elapsed:.1f} ç§’")
        print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {REPORT_FILE}")
        if total_skipped:
            print(f"æ— æ³•è¯†åˆ«çš„æ–‡ä»¶è®°å½•äº: {SKIPPED_LOG}")
    
    # ä¿å­˜ç¼“å­˜
    await save_cache(cache)

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='æ£€æŸ¥å‰§é›†æ˜¯å¦ç¼ºé›†å¹¶è®¢é˜…/ä¸‹è½½ç¼ºå¤±é›†æ•°')
    parser.add_argument('--show', type=str, help='åªå¤„ç†æŒ‡å®šçš„å‰§é›†åç§°ï¼Œä¾‹å¦‚ "å“¥è°­ (2014)"')
    parser.add_argument('--no-subscribe', action='store_true', help='ç¦ç”¨è‡ªåŠ¨è®¢é˜…åŠŸèƒ½')
    parser.add_argument('--download', action='store_true', help='å¯ç”¨è‡ªåŠ¨ä¸‹è½½åŠŸèƒ½')
    parser.add_argument('--subscribe-all', action='store_true', help='è®¢é˜…æ‰€æœ‰ç¼ºå¤±å‰§é›†ï¼Œæ— è®ºç¼ºå¤±å‡ é›†')
    parser.add_argument('--threshold', type=int, help='è®¾ç½®è®¢é˜…é˜ˆå€¼ï¼Œç¼ºå¤±è¶…è¿‡å¤šå°‘é›†æ—¶è®¢é˜…æ•´å­£')
    parser.add_argument('--force-check-all', action='store_true', help='å¼ºåˆ¶æ£€æŸ¥æ‰€æœ‰å‰§é›†ï¼ŒåŒ…æ‹¬å·²è®°å½•ä¸ºå®Œæ•´çš„å‰§é›†')
    parser.add_argument('--merge-cache', type=str, help='åˆå¹¶æ—§ç¼“å­˜æ–‡ä»¶(å¦‚tmdb_cache.json)åˆ°æ–°ç¼“å­˜æ ¼å¼')
    
    # å­˜å‚¨ç±»å‹é€‰æ‹©
    storage_group = parser.add_argument_group('å­˜å‚¨ç±»å‹')
    storage_group.add_argument('--storage', type=str, choices=['rclone', 'alist', 'webdav', 'local'], 
                             help='é€‰æ‹©å­˜å‚¨ç±»å‹: rclone, alist, webdav, local')
    
    # å­˜å‚¨é…ç½®
    rclone_group = parser.add_argument_group('Rcloneé…ç½®')
    rclone_group.add_argument('--rclone-remote', type=str, help='Rcloneè¿œç¨‹è·¯å¾„')
    
    alist_group = parser.add_argument_group('Alisté…ç½®')
    alist_group.add_argument('--alist-url', type=str, help='AlistæœåŠ¡å™¨URL')
    alist_group.add_argument('--alist-username', type=str, help='Alistç”¨æˆ·å')
    alist_group.add_argument('--alist-password', type=str, help='Alistå¯†ç ')
    alist_group.add_argument('--alist-token', type=str, help='Alistè®¿é—®ä»¤ç‰Œ')
    alist_group.add_argument('--alist-path', type=str, help='Aliståª’ä½“è·¯å¾„')
    
    webdav_group = parser.add_argument_group('WebDAVé…ç½®')
    webdav_group.add_argument('--webdav-url', type=str, help='WebDAVæœåŠ¡å™¨URL')
    webdav_group.add_argument('--webdav-username', type=str, help='WebDAVç”¨æˆ·å')
    webdav_group.add_argument('--webdav-password', type=str, help='WebDAVå¯†ç ')
    webdav_group.add_argument('--webdav-path', type=str, help='WebDAVåª’ä½“è·¯å¾„')
    
    local_group = parser.add_argument_group('æœ¬åœ°é…ç½®')
    local_group.add_argument('--local-path', type=str, help='æœ¬åœ°åª’ä½“è·¯å¾„')
    
    args = parser.parse_args()
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°
    if args.no_subscribe:
        global AUTO_SUBSCRIBE
        AUTO_SUBSCRIBE = False
    if args.download:
        global AUTO_DOWNLOAD
        AUTO_DOWNLOAD = True
    if args.subscribe_all:
        global SUBSCRIBE_THRESHOLD
        SUBSCRIBE_THRESHOLD = 0
    if args.threshold is not None:
        SUBSCRIBE_THRESHOLD = args.threshold
    
    # è®¾ç½®å­˜å‚¨ç±»å‹
    if args.storage:
        global STORAGE_TYPE
        if args.storage == 'rclone':
            STORAGE_TYPE = StorageType.RCLONE
        elif args.storage == 'alist':
            STORAGE_TYPE = StorageType.ALIST
        elif args.storage == 'webdav':
            STORAGE_TYPE = StorageType.WEBDAV
        elif args.storage == 'local':
            STORAGE_TYPE = StorageType.LOCAL
    
    # åº”ç”¨å­˜å‚¨é…ç½®
    if args.rclone_remote:
        import utils.config
        utils.config.RCLONE_REMOTE = args.rclone_remote
        
    if args.alist_url:
        import utils.config
        utils.config.ALIST_URL = args.alist_url
    if args.alist_username:
        import utils.config
        utils.config.ALIST_USERNAME = args.alist_username
    if args.alist_password:
        import utils.config
        utils.config.ALIST_PASSWORD = args.alist_password
    if args.alist_token:
        import utils.config
        utils.config.ALIST_TOKEN = args.alist_token
    if args.alist_path:
        import utils.config
        utils.config.ALIST_PATH = args.alist_path
        
    if args.webdav_url:
        import utils.config
        utils.config.WEBDAV_URL = args.webdav_url
    if args.webdav_username:
        import utils.config
        utils.config.WEBDAV_USERNAME = args.webdav_username
    if args.webdav_password:
        import utils.config
        utils.config.WEBDAV_PASSWORD = args.webdav_password
    if args.webdav_path:
        import utils.config
        utils.config.WEBDAV_PATH = args.webdav_path
        
    if args.local_path:
        import utils.config
        utils.config.LOCAL_PATH = args.local_path
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå¹¶ç¼“å­˜
    if args.merge_cache:
        asyncio.run(merge_old_cache(args.merge_cache))
        return
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡ç½®å®Œæ•´å‰§é›†ç¼“å­˜
    if args.force_check_all:
        asyncio.run(reset_cache())
    
    # è¿è¡Œä¸»ç¨‹åº
    if args.show:
        asyncio.run(main_async(specific_show=args.show))
    else:
        asyncio.run(main_async())

if __name__ == "__main__":
    main() 